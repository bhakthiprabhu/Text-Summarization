{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80c868c1",
   "metadata": {},
   "source": [
    "# Project : Text Summarization\n",
    "\n",
    "Text summarization is the technique for generating a concise and precise summary of voluminous texts while focusing on the sections that convey useful information, and without losing the overall meaning.\n",
    "\n",
    "Automatic text summarization aims to transform lengthy documents into shortened versions, something which could be difficult and costly to undertake if done manually.\n",
    "\n",
    "Machine learning algorithms can be trained to comprehend documents and identify the sections that convey important facts and information before producing the required summarized texts. \n",
    "\n",
    "In this example, we want to summarize the information found on Wikipedia article."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40dc47c",
   "metadata": {},
   "source": [
    "# Two simple ways to scrape text from Wikipedia\n",
    "\n",
    "1) Easy way: wikipedia library\n",
    "\n",
    "2) General way: urllib and BeautifulSoup libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba15aa6",
   "metadata": {},
   "source": [
    "1) Easy way: wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e4a807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import wikipedia\n",
    "\n",
    "# Specify the title of the Wikipedia page\n",
    "wiki = wikipedia.page('Elon Reeve Musk')\n",
    "\n",
    "# Extract the plain text content of the page, excluding images, tables, and other data.\n",
    "text = wiki.content\n",
    "\n",
    "# Replace '==' with '' (an empty string)\n",
    "text = text.replace('==', '')\n",
    "\n",
    "# Replace '\\n' (a new line) with '' & end the string at $1000.\n",
    "text = text.replace('\\n', '')[:-12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48f91be",
   "metadata": {},
   "source": [
    "2) General way: urllib and BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c5e3182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as BeautifulSoup\n",
    "import urllib.request  \n",
    "import re\n",
    "\n",
    "# Fetching the content from the URL\n",
    "fetched_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Elon_Musk')\n",
    "\n",
    "article_read = fetched_data.read()\n",
    "\n",
    "# Parsing the URL content and storing in a variable\n",
    "article_parsed = BeautifulSoup.BeautifulSoup(article_read,'html.parser')\n",
    "\n",
    "# Returning <p> tags\n",
    "paragraphs = article_parsed.find_all('p')\n",
    "\n",
    "article_content = ''\n",
    "\n",
    "# Looping through the paragraphs and adding them to the variable\n",
    "for p in paragraphs:  \n",
    "    article_content += p.text\n",
    "    \n",
    "# Drop footnote superscripts in brackets\n",
    "text = re.sub(r\"\\[.*?\\]+\", '', article_content)\n",
    "\n",
    "# Replace '\\n' (a new line) with '' and end the string at $1000.\n",
    "text = text.replace('\\n', '')[:-11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95ae005",
   "metadata": {},
   "source": [
    "# Step 1 : Preparing the data \n",
    "\n",
    "Here We're using beautifulSoup library\n",
    "\n",
    "The BeautifulSoup library is used for parsing the page while the urllib library is used for connecting to the page and retrieving the HTML.\n",
    "BeautifulSoup converts the incoming text to Unicode characters and the outgoing text to UTF-8 characters, saving you the hassle of managing different charset encodings while scraping text from the web.\n",
    "\n",
    "We’ll use the urlopen function from the urllib.request utility to open the web page. Then, we’ll use the read function to read the scraped data object. \n",
    "\n",
    "For parsing the data, we’ll call the BeautifulSoup object and pass two parameters to it; that is, the article_read and the html.parser.\n",
    "\n",
    "The find_all function is used to return all the <p> elements present in the HTML. Furthermore, using .text enables us to select only the texts found within the <p> elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5f740b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as BeautifulSoup\n",
    "import urllib.request  \n",
    "\n",
    "# Fetching the content from the URL\n",
    "fetched_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Elon_Musk')\n",
    "\n",
    "article_read = fetched_data.read()\n",
    "\n",
    "# Parsing the URL content and storing in a variable\n",
    "article_parsed = BeautifulSoup.BeautifulSoup(article_read,'html.parser')\n",
    "\n",
    "# Returning <p> tags\n",
    "paragraphs = article_parsed.find_all('p')\n",
    "\n",
    "article_content = ''\n",
    "\n",
    "# Looping through the paragraphs and adding them to the variable\n",
    "for p in paragraphs:  \n",
    "    article_content += p.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c7478a",
   "metadata": {},
   "source": [
    "# Step 2 : Processing the data \n",
    "\n",
    "To ensure the scrapped textual data is as noise-free as possible, we’ll perform some basic text cleaning.  \n",
    "To assist us to do the processing, we’ll import a list of stopwords from the nltk library.\n",
    "Stop words are a set of commonly used words in any language. For example, in English, “the”, “is” and “and”, would easily qualify as stop words.\n",
    "\n",
    "We’ll also import PorterStemmer, which is an algorithm for reducing words into their root forms. For example, cleaning, cleaned, and cleaner can be reduced to the root clean.\n",
    "\n",
    "Furthermore, we’ll create a dictionary table having the frequency of occurrence of each of the words in the text. We’ll loop through the text and the corresponding words to eliminate any stop words.\n",
    "\n",
    "Then, we’ll check if the words are present in the frequency_table. If the word was previously available in the dictionary, its value is increased by 1. Otherwise, if the word is recognized for the first time, its value is set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79d66834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "def _create_dictionary_table(text_string) -> dict:\n",
    "\n",
    "    # Removing stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = word_tokenize(text_string)\n",
    "    \n",
    "    # Reducing words to their root form\n",
    "    stem = PorterStemmer()\n",
    "    \n",
    "    # Creating dictionary for the word frequency table\n",
    "    frequency_table = dict()\n",
    "    for wd in words:\n",
    "        wd = stem.stem(wd)\n",
    "        if wd in stop_words:\n",
    "            continue\n",
    "        if wd in frequency_table:\n",
    "            frequency_table[wd] += 1\n",
    "        else:\n",
    "            frequency_table[wd] = 1\n",
    "\n",
    "    return frequency_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab5029b",
   "metadata": {},
   "source": [
    "# Step 3:  Tokenizing the article into sentences\n",
    "\n",
    "Tokenizing the sentences is done to get all the words present in the sentences.\n",
    "\n",
    "To split the article_content into a set of sentences, we’ll use the built-in method from the nltk library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e786fdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(article_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c2754a",
   "metadata": {},
   "source": [
    "# Step 4: Finding the weighted frequencies of the sentences\n",
    "\n",
    "To evaluate the score for every sentence in the text, we’ll be analyzing the frequency of occurrence of each term.\n",
    "In this case, we’ll be scoring each sentence by its words; that is, adding the frequency of each important word found in the sentence.\n",
    "\n",
    "Importantly, to ensure long sentences do not have unnecessarily high scores over short sentences, we divided each score of a sentence by the number of words found in that sentence.\n",
    "\n",
    "Also, to optimize the dictionary’s memory, we arbitrarily added sentence[:7], which refers to the first 7 characters in each sentence. However, for longer documents, where you are likely to encounter sentences with the same first n_chars, it’s better to use hash functions or smart index functions to take into account such edge-cases and avoid collisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46547290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_sentence_scores(sentences, frequency_table) -> dict:   \n",
    "\n",
    "    # Algorithm for scoring a sentence by its words\n",
    "    sentence_weight = dict()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_wordcount = (len(word_tokenize(sentence)))\n",
    "        sentence_wordcount_without_stop_words = 0\n",
    "        for word_weight in frequency_table:\n",
    "            if word_weight in sentence.lower():\n",
    "                sentence_wordcount_without_stop_words += 1\n",
    "                if sentence[:7] in sentence_weight:\n",
    "                    sentence_weight[sentence[:7]] += frequency_table[word_weight]\n",
    "                else:\n",
    "                    sentence_weight[sentence[:7]] = frequency_table[word_weight]\n",
    "\n",
    "        sentence_weight[sentence[:7]] = sentence_weight[sentence[:7]] / sentence_wordcount_without_stop_words\n",
    "      \n",
    "    return sentence_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eb7326",
   "metadata": {},
   "source": [
    "# Step 5: Calculating the threshold of the sentences\n",
    "\n",
    "To further tweak the kind of sentences eligible for summarization, we’ll create the average score for the sentences. With this threshold, we can avoid selecting the sentences with a lower score than the average score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d654e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_average_score(sentence_weight) -> int:\n",
    "   \n",
    "    # Calculating the average score for the sentences\n",
    "    sum_values = 0\n",
    "    for entry in sentence_weight:\n",
    "        sum_values += sentence_weight[entry]\n",
    "\n",
    "    # Getting sentence average value from source text\n",
    "    average_score = (sum_values / len(sentence_weight))\n",
    "\n",
    "    return average_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f05a97f",
   "metadata": {},
   "source": [
    "# Step 6: Getting the summary\n",
    "\n",
    "Lastly, since we have all the required parameters, we can now generate a summary for the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aec32ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_article_summary(sentences, sentence_weight, threshold):\n",
    "    sentence_counter = 0\n",
    "    article_summary = ''\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if sentence[:7] in sentence_weight and sentence_weight[sentence[:7]] >= (threshold):\n",
    "            article_summary += \" \" + sentence\n",
    "            sentence_counter += 1\n",
    "\n",
    "    return article_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b2e2d4",
   "metadata": {},
   "source": [
    "#  Step 7: Final Step \n",
    "\n",
    "combining the above steps from 2 to 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c32b03c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_article_summary(article):\n",
    "    \n",
    "    #Processing the data\n",
    "    frequency_table = _create_dictionary_table(article)\n",
    "\n",
    "    #Tokenizing the article into sentences\n",
    "    sentences = sent_tokenize(article)\n",
    "\n",
    "    # Finding the weighted frequencies of the sentences\n",
    "    sentence_scores = _calculate_sentence_scores(sentences, frequency_table)\n",
    "\n",
    "    #Calculating the threshold of the sentences\n",
    "    threshold = _calculate_average_score(sentence_scores)\n",
    "\n",
    "    #Getting the summary\n",
    "    article_summary = _get_article_summary(sentences, sentence_scores, 1.5 * threshold)\n",
    "\n",
    "    return article_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1fefd0",
   "metadata": {},
   "source": [
    "# Step 8 : Print the Summary of data scapped using BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78d5b18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [5][6]\n",
      "Musk was born in Pretoria, South Africa, where he grew up. [9] Musk has British and Pennsylvania Dutch ancestry. [14][17]\n",
      "Musk's family was wealthy during his youth. [18][19][20] After his parents divorced in 1980, Musk mostly lived with his father. [21] He has a paternal half-sister and a half-brother. [18][22]\n",
      "In his biography, Ashlee Vance described Musk as an awkward and introverted child. [33] In 1990, he entered Queen's University in Kingston, Ontario. [53] Musk then returned as CEO of the merged company. [109] A cross-over, the Model X was launched in 2015. [170]\n",
      "In 2017, Musk founded The Boring Company to construct tunnels. [199]\n",
      "As early as 2017, Musk expressed interest in buying Twitter. [265]\n",
      "At the start of 2020, Musk had a net worth of $27 billion. [267] During this, Musk's net worth was often volatile. [404] After his death, the couple decided to use IVF to continue their family. [408]\n",
      "In 2008, Musk began dating English actress Talulah Riley. [417]\n",
      "In 2018, Musk and Canadian musician Grimes revealed that they were dating. \"[1] Later that month, Grimes tweeted that she and Musk had broken up again but remained on good terms. [434] Musk denied the report. [469]\n"
     ]
    }
   ],
   "source": [
    "summary_results = _run_article_summary(article_content)\n",
    "print(summary_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
